\documentclass{dialogue}

\begin{document}

\begin{otherlanguage}{english}
\begin{center}
{\Large\bfseries{Headline generation:\\first sentence vs neural machine translation}}

\medskip

Churikov N. S. (\texttt{nikita@chur.ru}),\\Sannikova E. (\texttt{elena.sannikova59@gmail.com})

\medskip

Saint Petersburg State University, Saint Petersburg, Russia\\
Behavox, Saint Petersburg, Russia
\end{center}

In this article, we describe our experience of participating
in a headline generation contest organized by VKontakte. We took the third place
in the competition by modifying the baseline solution through the data cleaning.
In addition, we tried to train and apply the transformer architecture combined
with byte pair encoding, but this solution turned out to be worse than the baseline.
At the end, we present our results on leaderboard for different solutions, and ROUGE
scores on our test set.\medskip

\textbf{Key words:} text summarization, headline generation, Russian language
\end{otherlanguage}

\bigskip

\begin{otherlanguage}{russian}
\begin{center}
{\Large\bfseries{Генерация заголовков:\\первое предложение против глубокого машинного перевода}}

\medskip

Чуриков Н. С. (\texttt{nikita@chur.ru}),\\Санникова Е. (\texttt{elena.sannikova59@gmail.com}),

\medskip

Санкт-Петербургский государственный Университет, Санкт-Петербург, Россия\\
Behavox, Санкт-Петербург, Россия
\end{center}

В данной статье мы описываем наш опыт участия в конкурсе по генерации заголовков,
организованном ВКонтакте. Мы заняли третье место в соревновании модифицировав
пороговое решение через очистку от лишней информации. Это позволило нам перевалить
за решение-baseline. Помимо этого, мы попробовали обучить и применить трансформер
токенизируя данные через byte pair encoding, однако это решение оказалось хуже
порогового. В конце мы приводим численные результаты, полученные в соревновании,
а также оценки, полученные на нашей тестовой выборке.\medskip

\textbf{Ключевые слова:} автореферирование текстов, генерация заголовков, русский язык
\end{otherlanguage}

\selectlanguage{english}

\section{Introduction}
The problem of generating headers is a specific summarization problem, where output of a model is just a short single sentence. Conceptually, there are two approaches to the problem: extractive and abstractive summarization. In the case of extractive summarization, only the original text is used. Abstractive summarization is different in that the output text may contain words that were not in the source material.

In this article, we describe our results in the header generation competition, which was organized by VKontakte for Dialog Evaluation. In our work, we used extractive and abstractive summarization. The methods used by us are described in the experiments section, the results of our models on the leaderboard and on our test set are given in the results section. At the end of the work, we present our conclusions on the competition and our thoughts on what can be improved.

\section{System description}
In the headline generation competition, we've decided to compare classical summarization approaches (textrank \cite{DBLP:journals/corr/BarriosLAW16}), which do not require training and approaches based on neural networks. As a result, we've managed to get the results in the leaderboard only for part of our experiments, specifically for the first sentence, transformer-architecture \cite{DBLP:journals/corr/VaswaniSPUJGKP17} with BPE trained on Wikipedia \cite{heinzerling2018bpemb} and trained on the competition data. We used the implementation of the transformer from the library of Open Neural Machine Translation \cite{2017opennmt}. The launch code for training, predictions, and models can be found in our repository \footnote{https://github.com/kuparez/headline\_generator}.

\section{Data and training}
Competition data was provided in the form of raw pieces of original html pages from Ria News. This means that there were various html tags and entities present in the data. And each news and a matching headline in raw text.

\begin{verbatim}
<p> <strong> <\strong> <\p> \n <p> <strong> Moscow, Dec 1 &nbsp; &mdash;
RIA news. <\strong> a fire in &nbsp; one of the &nbsp; workshops in
&nbsp;...<\p>
\end{verbatim}

As a result, the first thing we tried was to clean up the data from unnecessary information. So, we've made a preprocessor that removes all html tags and entities.

In addition, we've found that sometimes there is no text in the data, since the original news is represented by some form of image (for example, a screenshot of Twitter), and the news is purely Polish. These are all outliers we cleared data from.

However, if our models got empty string during inference, we used most common word in headlines which was "в".

Then, for training and validation of transformer, we've split the data in such a way that 90\% of data goes for training and 10\% for test. For test set creation we just shuffled data. For reproduction purposes we provide in our repository test set file.

\section{Experiments}
For all experiments, we used the preprocessing as described above. For training of our models we used 8 Nvidia K80.

\subsection{Evaluation}


\subsection{First sentence}

As described in \cite{Putra2018IncorporatingTS}, the first sentence for automatically creating a news headline is a very strong baseline. In particular, in the headline generation competition, the first sentence of news articles was proposed as a baseline solution.

However, we used our knowledge about the data and besides removal of html tags and entities we've also skipped first sentences with "риа новости" in it. This allowed us to skip sentences with information about the date the news was posted as these sentences did not contain any information besides that.

\subsection{Textrank}

We used the classic extractive summarization algorithm - textrank \cite{DBLP:journals/corr/BarriosLAW16}. We took implementation of this algorithm from gensim \cite{rehurek_lrec} To create a summary using keywords and original sentences we've set extraction size to be 20\% of the original text.

\subsection{Transformer}

Since the competition was held for only a month, we've decided to repeat the results described in the article VKontakte \cite{gavrilov2018self}. We took the implementation of the transformer from Open NMT, with the parameters described in the article.

Then, we tried to focus on what to use as input for the model. We tried the first sentence according to the rules above. We've also tried as an input first 2000 BPE tokens.

Due to restrictions on VK servers we could not make very complex predictions. There for, during predictions on leaderboard we've used beam search of size 5. But for evaluation on our test set we used beam size of 10.

\section{Results}

As could be seen in Table 1, none of the models we've trained could show better results than the preprocessed first sentence. Unfortunately, we could not check some of our models on VK servers due to the restrictions on time complexity of our models. For this reason, we also present results on our test set.

\begin{table}[htbp]
  \centering
  \begin{tabular}{|l|l|}
    \hline
    Algorithm                                                                                            & Score      \\ \hline
    Baseline                                                                                             & 0.19500071 \\ \hline
    First sentence                                                                                       & 0.19502427 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Wiki BPE \\ transformer\\ (first sentence, no beam search=5)\end{tabular} & 0.16397515 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Ria BPE \\ transformer\\ (2000 first tokens, beam search=5)\end{tabular}  & 0.16131584 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Textrank \\ summarization\end{tabular}                                    & 0.10764881 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Textrank\\ keywords\end{tabular}                                          & 0.06259589 \\ \hline
  \end{tabular}
  \caption{Results from evaluation by VK servers. Score is mean of ROUGE-1,2,L F1 score.}
\end{table}

Table 2 show us weird results. For some reason, on our split, out neural models not only better than first sentence model, but also show better results than VK server evaluation. So we decided to check for data leaks. We assumed that there could be none, because for train\/test split we used scikit learn \cite{scikit-learn} train test split method. So we decided to check, how many texts in our test set are equal to texts in train. It turned out that 975 texts in test set were found in train set.

Then we decided to check original data and find how many training examples are identical. We found, that 2651 training examples are identical.

\begin{table}[htbp]
  \centering
  \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
    \hline
    Algorithm\textbackslash{}Score                                                             & 1F   & 1P   & 1R   & 2F   & 2P   & 2R   & LF   & LP   & LR   \\ \hline
    First sentence                                                                             & 0.23 & 0.16 & 0.44 & 0.10 & 0.07 & 0.21 & 0.16 & 0.15 & 0.40 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Wiki BPE \\ transformer\\ (first sentence, beam=10)\end{tabular}& 0.37 & 0.39 & 0.36 & 0.20 & 0.21 & 0.19 & 0.34 & 0.37 & 0.34 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Wiki BPE \\ transformer\\ (first sentence, beam=5)\end{tabular} & 0.39 & 0.41 & 0.39 & 0.22 & 0.23 & 0.22 & 0.37 & 0.39 & 0.37 \\ \hline
    % \begin{tabular}[c]{@{}l@{}}Wiki BPE \\ transformer\\ (first sentence, beam=1)\end{tabular} & 0.37 & 0.37 & 0.37 & 0.20 & 0.20 & 0.20 & 0.34 & 0.36 & 0.35 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Ria BPE \\ transformer\\ (2000 first tokens)\end{tabular}       & 0.36 & 0.37 & 0.35 & 0.18 & 0.20 & 0.18 & 0.33 & 0.36 & 0.33 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Textrank \\ summarization\end{tabular}                          & 0.14 & 0.09 & 0.41 & 0.05 & 0.03 & 0.17 & 0.09 & 0.08 & 0.38 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Textrank\\ keywords\end{tabular}                                & 0.09 & 0.07 & 0.18 & 0.00 & 0.00 & 0.01 & 0.05 & 0.05 & 0.14 \\ \hline
    \end{tabular}
  \caption{ROUGE-1,2,F1, precision and recall scores.}
\end{table}

After our discovery of a leak in our test set distribution, we decided to check performance of our best abstractive summarization model, transformer trained using BPE trained on Wikipedia with beam search equal to 5 on dataset with train set leak removed.

\section{Related work}
The main article, that we've used as an inspiration, was the article VKontakte \cite{gavrilov2018self}. From there, we got the idea to use BPE \cite{DBLP:journals/corr/SennrichHB15}, as well as which hyperparameters to use for training of transformer. Then, we've started to browse their citations and we've found that there were a some criticism about what to use as an input to abstractive headline generation algorithms. As a result, we stumbled upon an article where the choice of the first sentence as an input to models was being criticized and authors discussed ways to extract so-called Topic Sentence \cite{Putra2018IncorporatingTS} is proposed.

\section{Conclusion and future work}
We have seen that the first sentence is a very strong baseline in the problem of generating headers. We have tried various options for preprocessing data, but none of them helped us improve the results of transformer.

We think that in future work it is worth trying to generate a topic sentence on the 5W1H principle described in \cite{Putra2018IncorporatingTS}, and the problem may lie in the wrong choice of model hyper parameters.


\color{blue}\section*{References}

\makeatletter
\renewcommand{\section}{\@gobbletwo}
\makeatother
\bibliography{\jobname}

\end{document}
