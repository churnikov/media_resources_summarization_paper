\documentclass{dialogue}

\begin{document}

\begin{otherlanguage}{english}
\begin{center}
{\Large\bfseries{Headline generation:\\first sentence vs neural machine translation}}

\medskip

Churikov N. S. (\texttt{nikita@chur.ru}),\\Sannikova E. (\texttt{elena.sannikova59@gmail.com})

\medskip

Saint Petersburg State University, Saint Petersburg, Russia\\
Behavox, Saint Petersburg, Russia
\end{center}

In this article, we describe our experience of participating
in a headline generation contest organized by VKontakte. We took the third place
in the competition by modifying the baseline solution through the data cleaning.
In addition, we tried to train and apply the transformer architecture combined
with byte pair encoding, but this solution turned out to be worse than the baseline.
At the end, we present our results on leaderboard for different solutions, and ROUGE
scores on our test set.\medskip

\textbf{Key words:} text summarization, headline generation, Russian language
\end{otherlanguage}

\bigskip

\begin{otherlanguage}{russian}
\begin{center}
{\Large\bfseries{Генерация заголовков:\\первое предложение против глубокого машинного перевода}}

\medskip

Чуриков Н. С. (\texttt{nikita@chur.ru}),\\Санникова Е. (\texttt{elena.sannikova59@gmail.com}),

\medskip

Санкт-Петербургский государственный Университет, Санкт-Петербург, Россия\\
Behavox, Санкт-Петербург, Россия
\end{center}

В данной статье мы описываем наш опыт участия в конкурсе по генерации заголовков,
организованном ВКонтакте. Мы заняли третье место в соревновании модифицировав
пороговое решение через очистку от лишней информации. Это позволило нам перевалить
за решение-baseline. Помимо этого, мы попробовали обучить и применить трансформер
токенизируя данные через byte pair encoding, однако это решение оказалось хуже
порогового. В конце мы приводим численные результаты, полученные в соревновании,
а также оценки, полученные на нашей тестовой выборке.\medskip

\textbf{Ключевые слова:} автореферирование текстов, генерация заголовков, русский язык
\end{otherlanguage}

\selectlanguage{english}

\section{Introduction}
Describe (briefly!):
\begin{enumerate}
    \item the task and the dataset
    \item your approach and cite some major works, it was based on
    \item the structure of your paper
\end{enumerate}

\section{System description}
In the headline generation competition, we've decided to compare classical summatization approaches (textrank), which do not require training and approaches based on neural networks. As a result, we've managed to get the results in the leaderboard only for part of our experiments, specifically for the first sentence, transformer-architecture with bpe \cite{} trained on Wikipedia \cite{} and trained on the competition data. We used the implementation of the transformer from the library of Open Neural Machine Translation \cite{}. The launch code for training, predictions, and models can be found in our repository \footnote{https://github.com/kuparez/headline\_generator}.

\section{Data and training}
Competition data was provided in the form of raw pieces of original html pages from Ria News. This means that there were various html tags and entities present in the data. And each news and a matching headline in raw text.

\begin{verbatim}
<p> <strong> <\strong> <\p> \n <p> <strong> Moscow, Dec 1 &nbsp; &mdash;
RIA news. <\strong> a fire in &nbsp; one of the &nbsp; workshops in
&nbsp;...<\p>
\end{verbatim}

As a result, the first thing we tried was to clean up the data from unnecessary information. So, we've made a preprocessor that removes all html tags and entities.

In addition, we've found that sometimes there is no text in the data, since the original news is represented by some form of image (for example, a screenshot of Twitter), and the news is purely Polish. These are all outliers we cleared data from.

Then, for training and validation of transformer, we've split the data in such a way that 90\% of data goes for training and 10\% for test.

\section{Experiments}
For all experiments, we used the preprobe as described above. Training models produced on 8 Nvidia K80.

\subsection{First sentence}

As described in [cite], the first sentence for automatically creating a news headline is a very strong baseline. In particular, and in the ongoing competition, the first offer of news was proposed by the threshold decision.

However, we used substantive knowledge about the data and as the first sentence we used the one in which there is no “ria news”.

\subsection{Textrank}

We used the classic extractive summatization algorithm - textrank \cite{}. To create a summary using keywords and original sentences we've set extraction size to be 20\% of the original text.

\subsection{Transformer}

Since the competition was held for only a month, we've decided to repeat the results described in the article VKontakte \cite{}. We took the implementation of the transformer from Open NMT, with the parameters described in the article.

Then, we tried to focus on what to use as input for the model. We tried the first sentence according to the rules above. We've also tried as an input first 2000 bpe tokens.

\section{Results}

As a result, none of the models we've trained could show better results than the first sentence. Unfortunately, we could not check some of our models on VK servers due to the restrictions on time complexity of our models. For this reason, we also present results on our test set.

\begin{table}[]
  \centering
  \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
    \hline
    Algorithm\textbackslash{}Score                                                       & 1F   & 1P   & 1R   & 2F   & 2P   & 2R   & LF   & LP   & LR   \\ \hline
    First sentence                                                                       & 0.23 & 0.16 & 0.44 & 0.10 & 0.07 & 0.21 & 0.16 & 0.15 & 0.40 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Wiki BPE \\ transformer\\ (first sentence)\end{tabular}   & 0.37 & 0.39 & 0.36 & 0.20 & 0.21 & 0.19 & 0.34 & 0.37 & 0.34 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Ria BPE \\ transformer\\ (2000 first tokens)\end{tabular} & 0.36 & 0.37 & 0.35 & 0.18 & 0.20 & 0.18 & 0.33 & 0.36 & 0.33 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Textrank \\ summarization\end{tabular}                    & 0.14 & 0.09 & 0.41 & 0.05 & 0.03 & 0.17 & 0.09 & 0.08 & 0.38 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Textrank\\ keywords\end{tabular}                          & 0.09 & 0.07 & 0.18 & 0.00 & 0.00 & 0.01 & 0.05 & 0.05 & 0.14 \\ \hline
    \end{tabular}
  \caption{ROUGE-1,2,F1, precision and recall scores.}
\end{table}

\begin{table}[]
  \centering
  \begin{tabular}{|l|l|}
    \hline
    Algorithm                                                                                            & Score      \\ \hline
    Baseline                                                                                             & 0.19500071 \\ \hline
    First sentence                                                                                       & 0.19502427 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Wiki BPE \\ transformer\\ (first sentence, no beam search)\end{tabular}   & 0.16397515 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Ria BPE \\ transformer\\ (2000 first tokens, no beam search)\end{tabular} & 0.16131584 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Textrank \\ summarization\end{tabular}                                    & 0.10764881 \\ \hline
    \begin{tabular}[c]{@{}l@{}}Textrank\\ keywords\end{tabular}                                          & 0.06259589 \\ \hline
  \end{tabular}
  \caption{Results from evaluation by VK servers.}
\end{table}

Competition used mean of ROUGE-1,2,L F1 score.

\section{Related work}
The main article, that we've used as an inspiration, was the article VKontakte \cite{}. From there, we got the idea to use bpe \cite{}, as well as which hyperparameters to use for training of transformer \cite{}. Then, we've started to browse their citations and we've found that there were a some criticism about what to use as an input to abstractive headline generation algorithms. As a result, we stumbled upon an article where the choice of the first sentence as an input to models was being criticised and authors discussed ways to extract so-called Topic Sentence \cite{} is proposed.

\section{Conclusion and future work}
We have seen that the first sentence is a very strong baseline in the problem of generating headers. We have tried various options for preprocessing data, but none of them helped us improve the results of transformer.

We think that in future work it is worth trying to generate a topic sentence on the 5W1H principle described in \cite{}, and the problem may lie in the wrong choice of model hyper parameters.


\color{blue}\section*{References}

\makeatletter
\renewcommand{\section}{\@gobbletwo}
\makeatother
\bibliography{\jobname}

\end{document}
