## Introduction
Проблема генерации заголовков является специфичной задачей суммаризации, где краткое описание состоит из одного предложения. Концептуально существует два подхода к задаче: экстрактивная и абстрактивная суммаризация. В случае экстрактивной суммаризации используется только оригинальный текст вплоть до переиспользования предложений. Абстрактивная суммаризация отличается тем, что в тексте на выходе могут быть слова, которых не было в исходном материале.

В данной статье мы описываем наши результаты в соревновании по генерации заголовков, которое было организовано ВКонтакте в рамках Dialog Evaluation. В работе мы использовали экстрактивную и абстрактивную суммаризации. Методы, использованные нами, описаны в разделе experiments, результаты наших моделей на таблице лидеров и на нашем тестовом наборе данных приводятся в разделе Results. В конце работы мы приводим наши выводы по соревнованию и наши мысли по поводу того, что возможно улучшить.

## System description
<!-- This section is devoted to the detailed description of your contribution. The architectures and the methods should be presented here. Try to make your explanation as clear as possible for those who would desire to reproduce your approach.\footnote{https://github.com/kuparez/headline_generator} -->
В соревновании по генерации заголовков, мы поставили перед собой задачу сравнить классические подходы к суммаризации (textrank), которые не требуют обучения и подходы основанные на нейронных сетях. Как результат, у нас получилось получить результат в таблице лидеров только по части наших экспериментов, а конкретно по первому предложению, transformer-architecture вместе с bpe [ссылка на sentpiece], обученным на википедии [ссылка-на-библиотеку] и обученным на данных соревнования. Мы воспользовались реализацией трансформера из библиотеки Open Neural Machine Translation [ссылка]. Код запуска обучения, предсказания, а также модели можно найти в нашем репозитории `\footnote{https://github.com/kuparez/headline_generator}`.

## Data and training
<!-- In this section describe anything related to the prepossessing of the dataset, pretrained embeddings and language models you used and the details of the training procedure. -->
В соревновании данные были предоставлены в виде  сырых вырезанных кусков html страниц Риа Новостей. Это значит, а данных присутствовали различные html тэги и сущности. И каждой новости был сопоставлен заголовок.

```
<p><strong><\/strong><\/p>\n<p><strong>москва, 1 дек&nbsp;&mdash; риа новости.<\/strong> пожар в&nbsp;одном из&nbsp;цехов в&nbsp;...<\/p>
```

В результате, первое, что мы попробовали сделать -- это отчистить данных от лишней информации. Там образом, мы сделали предобработчик, который убирает все html тэги и сущности.

Помимо этого, мы обнаружили, что в данных, порой отсутствует текст, поскольку оригинальная новость представлена какой-то формой изображения (скриншот твиттера, например), а также в новостях есть новость чисто на польском. Это все выбросы, от которых мы почистили данные.

Затем, для обучения transformer-а и валидации мы разбили данные в соотношении 90% -- train, 10% test.

## Experiments
<!-- This section is devoted to the description of your experiment settings. -->
Для всех экспериментов мы использовали предобратобку, описанную выше. Обучение моделей призводилось на 8 Nvidia K80.

### Первое предложение
Как описано в статье [cite], первое предложение для автоматического создания новостного заголовка является очень сильным бэйзланом. В частности и в проводимом соревновании, первое предложение новости было предложено порогововым решением.

Однако мы воспользовались предметным знанием о данных и в качестве первого предложения использовали то, в котором отсутствует "риа новости".

### Textrank
Мы воспользовались классическим алгоритмом экстрактивной суммаризации -- textrank. Для создания summary было задано извлекать 20% оригинального текста.

Помимо этого, мы решили попробовать извлекать ключевые слова. В его случае мы посчитали средний размер заголовка. Это значение оказалось равным 9, так что мы указали это значение в качестве параметра, который отвечает за количество слов на выходе алгоритма.

### Transformer
Поскольку на соревнование был отведен, месяц, то мы, в первую очередь решили повторить результаты, описанные в статье ВКонтакте [cite]. Нами была взята реализация трансформера из Open NMT, с параметрами, описанными в статье.

Затем, мы попытались сфокусироваться на том, что использовать в качестве входа для модели. Мы попробовали первое предложение, по принципу выше, выделять summary с помощью textrank, брать первые 2000 символов.

## Results
<!-- This section presents the results of your experiments. -->
В результате, ни одна из обученных нами моделей не смогла показать результаты лучше чем первое предложение. К сожалению, часть решений нам даже не удалось проверить, поскольку они оказались слишком вычислительно долгими. По этой причине мы приводим численные результаты на нашем разбиении данных.

\[Тут будет таблица с численными результатами на Leaderboard-е\]

|Метод                 |Score         |
|----------------------|--------------|
|Baseline              |0.19500071    |
|First sentence        |__0.19502427__|
|Wiki BPE transformer  |0.16397515    |
|Ria BPE transformer   |0.16131584    |
|Textrank summarization|0.10764881    |
|Textrank keywords     |0.06259589    |

\[Тут будет таблица с численными результатами на наших тестовых данных\]

|Метод                                   |ROUGE1F   |ROUGE1P   |ROUGE1R   |ROUGE2F   |ROUGE2P   |ROUGE2R   |ROUGELF   |ROUGELP   |ROUGELR   |
|----------------------------------------|----------|----------|----------|----------|----------|----------|----------|----------|----------|
|Baseline                                |          |          |          |          |          |          |          |          |          |
|First sentence                          |0.23774997|0.16758911|0.44704444|0.10519344|0.07293198|0.21040550|0.16575322|0.15374833|0.40907362|
|Wiki BPE transformer(Первое предложение)|0.37487193|0.39485263|0.36384412|0.20615759|0.21835507|0.19987960|0.34827759|0.37437382|0.34495650|
|Wiki BPE transformer(Первое предложение, beam=5)|0.39718894|0.41043152|0.39189725|0.22824611|0.23624047|0.22560835|0.37109221|0.39005237|0.37248966|
|Wiki BPE transformer(Первое предложение, без beam search)|0.37237167|0.37945115|0.37267999|0.20512290|0.20917494|0.20587372|0.34740491|0.36006422|0.35372614|
|Ria BPE transformer(2000 первых токенов)|0.36007817|0.37974070|0.35128008|0.18932258|0.20127022|0.18439079|0.33378960|0.36071193|0.33362054|
|Textrank summarization                  |0.14772676|0.09561833|0.41946673|0.05500968|0.03480776|0.17501952|0.09239122|0.08765440|0.38756839|
|Textrank keywords                       |0.09385889|0.07032998|0.18420428|0.00729198|0.00543009|0.01560466|0.05880374|0.05680884|0.14598096|

## Related work
<!-- This section is obligatory. In this section describe key papers and ideas in the domain of text summariation and headline summarization and cite the works and implementations, if any, you used. Below you can find an instruction on how to cite a paper in the {\LaTeX} style of Dialogue papers. -->
Основная статья, от которой мы отталкивались, была статья вконтакте [cite]. Оттуда мы вязли идею применить bpe [cite], а также какие гиперпараметры указать при обучении transformer-а Open NMT [cite]. Дальше, мы пошли по ссылкам статьи и начали искать варианты, что может быть входом для модели. Как результат, мы наткнулись на статью, где критикуется выбор первого предложения в качестве входа моделей и предлагается способ поиска так называемого Topic Sentence [cite]. Таким образом, мы пришли к выводу, что надо искать способ передавать модели некоторую основную суть текста заранее. Так мы пришли к эксперименту с суммаризацией текста через textrank, а затем передавать этот сжатый текст генератору заголовков.

## Conclusion and future work
<!-- Draw a conclusion and provide some insights on how your approach can be improved. -->
Мы убедились, что первое предложение является очень сильным бэйзланом в задаче генерации заголовков. Нами были попробованы различные варианты предварительной обработки данных, но ни один из них не помог нам улучшить результаты transformer-а.

Мы думаем, что в дальнейшей работе стоит попробовать генерировать topic sentence по принципу 5W1H, описанному в [cite], а также проблема может крыться в неправильном выборе гиперпараметров модели.  
