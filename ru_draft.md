## System description
<!-- This section is devoted to the detailed description of your contribution. The architectures and the methods should be presented here. Try to make your explanation as clear as possible for those who would desire to reproduce your approach.\footnote{https://github.com/kuparez/headline_generator} -->
В соревновании по генерации заголовков, мы поставили перед собой задачу сравнить классические подходы к суммаризации (textrank), которые не требуют обучения и подходы основанные на нейронных сетях. Как результат, у нас получилось получить результат в таблице лидеров только по части наших экспериментов, а конкретно по первому предложению, transformer-architecture вместе с bpe [ссылка на sentpiece], обученным на википедии [ссылка-на-библиотеку] и обученным на данных соревнования. Мы воспользовались реализацией трансформера из библиотеки Open Neural Machine Translation [ссылка]. Код запуска обучения, предсказания, а также модели можно найти в нашем репозитории `\footnote{https://github.com/kuparez/headline_generator}`.

## Data and training
<!-- In this section describe anything related to the prepossessing of the dataset, pretrained embeddings and language models you used and the details of the training procedure. -->
В соревновании данные были предоставлены в виде  сырых вырезанных кусков html страниц Риа Новостей. Это значит, а данных присутствовали различные html тэги и сущности. И каждой новости был сопоставлен заголовок.

```
<p><strong><\/strong><\/p>\n<p><strong>москва, 1 дек&nbsp;&mdash; риа новости.<\/strong> пожар в&nbsp;одном из&nbsp;цехов в&nbsp;...<\/p>
```

В результате, первое, что мы попробовали сделать -- это отчистить данных от лишней информации. Там образом, мы сделали предобработчик, который убирает все html тэги и сущности.

Помимо этого, мы обнаружили, что в данных, порой отсутствует текст, поскольку оригинальная новость представлена какой-то формой изображения (скриншот твиттера, например), а также в новостях есть новость чисто на польском. Это все выбросы, от которых мы почистили данные.

Затем, для обучения transformer-а и валидации мы разбили данные в соотношении 90% -- train, 10% test.

## Experiments
<!-- This section is devoted to the description of your experiment settings. -->
Для всех экспериментов мы использовали предобратобку, описанную выше. Обучение моделей призводилось на 8 Nvidia K80.

### Первое предложение
Как описано в статье [cite], первое предложение для автоматического создания новостного заголовка является очень сильным бэйзланом. В частности и в проводимом соревновании, первое предложение новости было предложено порогововым решением.

Однако мы воспользовались предметным знанием о данных и в качестве первого предложения использовали то, в котором отсутствует "риа новости".

### Transformer
Поскольку на соревнование был отведен, месяц, то мы, в первую очередь решили повторить результаты, описанные в статье ВКонтакте [cite]. Нами была взята реализация трансформера из Open NMT, с параметрами, описанными в статье.

Затем, мы попытались сфокусироваться на том, что использовать в качестве входа для модели. Мы попробовали первое предложение, по принципу выше, выделять summary с помощью textrank, брать первые 2000 символов.

## Results
<!-- This section presents the results of your experiments. -->
В результате, ни одна из обученных нами моделей не смогла показать результаты лучше чем первое предложение. К сожалению, часть решений нам даже не удалось проверить, поскольку они оказались слишком вычислительно долгими. По этой причине мы приводим численные результаты на нашем разбиении данных.

\[Тут будет таблица с численными результатами на Leaderboard-е\]

\[Тут будет таблица с численными результатами на наших тестовых данных\]

## Related work
<!-- This section is obligatory. In this section describe key papers and ideas in the domain of text summariation and headline summarization and cite the works and implementations, if any, you used. Below you can find an instruction on how to cite a paper in the {\LaTeX} style of Dialogue papers. -->
Основная статья, от которой мы отталкивались, была статья вконтакте [cite]. Оттуда мы вязли идею применить bpe [cite], а также какие гиперпараметры указать при обучении transformer-а Open NMT [cite]. Дальше, мы пошли по ссылкам статьи и начали искать варианты, что может быть входом для модели. Как результат, мы наткнулись на статью, где критикуется выбор первого предложения в качестве входа моделей и предлагается способ поиска так называемого Topic Sentence [cite]. Таким образом, мы пришли к выводу, что надо искать способ передавать модели некоторую основную суть текста заранее. Так мы пришли к эксперименту с суммаризацией текста через textrank, а затем передавать этот сжатый текст генератору заголовков.

## Conclusion and future work
<!-- Draw a conclusion and provide some insights on how your approach can be improved. -->
Мы убедились, что первое предложение является очень сильным бэйзланом в задаче генерации заголовков. Нами были попробованы различные варианты предварительной обработки данных, но ни один из них не помог нам улучшить результаты transformer-а.

Мы думаем, что в дальнейшей работе стоит попробовать генерировать topic sentence по принципу 5W1H, описанному в [cite], а также проблема может крыться в неправильном выборе гиперпараметров модели.  
